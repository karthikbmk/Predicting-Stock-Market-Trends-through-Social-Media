{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**PREDICTING STOCK MARKET TRENDS THROUGH SOCIAL MEDIA**\n",
    "\n",
    "ADITYA SHIRODKAR\n",
    "\n",
    "A20332644\n",
    "\n",
    "All the data used for this project is available at http://cloud.aditya11.com\n",
    "\n",
    "Username: twitter\n",
    "\n",
    "Password: twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import glob\n",
    "import hashlib\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from urllib import urlretrieve\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    \"\"\" Return a list of file names in this directory that end in .txt \n",
    "    The list should be sorted alphabetically by file name.\n",
    "    Params:\n",
    "        path....a directory containing .txt review files.\n",
    "    Returns:\n",
    "        a list of .txt file names, sorted alphabetically.\n",
    "    \"\"\"\n",
    "    return [f for f in glob.glob(path + os.sep + \"*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_files = get_files('train/tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file2tweets(filename):\n",
    "    return io.open(filename, encoding='utf8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "for tweets in all_train_files:\n",
    "    temp.append(len(file2tweets(tweets)))\n",
    "\n",
    "OY = []\n",
    "total = 0\n",
    "for i in range(1, len(temp) + 1):\n",
    "    total += temp[i-1]\n",
    "    if i % 8 == 0:\n",
    "        OY.append(total)\n",
    "        total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "width = .35\n",
    "OX = [\"02NOV\", \"03NOV\", \"04NOV\", \"05NOV\", \"06NOV\", \"09NOV\", \"10NOV\", \"12NOV\", \"16NOV\", \"17NOV\", \"18NOV\", \"19NOV\", \"20NOV\", \"23NOV\", \"24NOV\", \"26NOV\", \"27NOV\"]\n",
    "ind = np.arange(len(OY))\n",
    "plt.bar(ind, OY)\n",
    "plt.xticks(ind + width / 2, OX)\n",
    "plt.xlabel('DATE')\n",
    "plt.ylabel('NO. OF TWEETS')\n",
    "\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "from matplotlib.finance import candlestick, quotes_historical_yahoo\n",
    "\n",
    "# (Year, Month, Day)\n",
    "FROM = (2015, 11, 02)\n",
    "TO = (2015, 11, 27)\n",
    "\n",
    "# (DATE, OPEN, CLOSE, HIGH, LOW, VOLUME)\n",
    "quotes = quotes_historical_yahoo('DJIA', FROM, TO)\n",
    "\n",
    "if len(quotes) == 0:\n",
    "    raise SystemExit\n",
    "\n",
    "fig = figure()\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "candlestick(ax, quotes, width=0.8)\n",
    "\n",
    "ax.xaxis_date()\n",
    "ax.autoscale_view()\n",
    "setp( gca().get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Given a string, return a list of tokens such that: (1) all\n",
    "    tokens are lowercase, (2) all punctuation is removed. Note that\n",
    "    underscore (_) is not considered punctuation.\n",
    "    UPDATE: To be more specific, a token is a sequence of \n",
    "    alphanumeric characters, i.e., [A-Za-z0-9_]. Non-ascii characters\n",
    "    are not considered to be part of tokens.\n",
    "    Params:\n",
    "        text....a string\n",
    "    Returns:\n",
    "        a list of tokens\n",
    "    \"\"\"\n",
    "    return [x.lower() for x in re.findall(r\"\\w+\", text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_vectorize(filenames, tokenizer_fn=tokenize, min_df=1,\n",
    "                 max_df=1., binary=True, ngram_range=(1,1)):\n",
    "    \"\"\"\n",
    "    Convert a list of filenames into a sparse csr_matrix, where\n",
    "    each row is a file and each column represents a unique word.\n",
    "    Use sklearn's CountVectorizer: http://goo.gl/eJ2PJ5\n",
    "    Params:\n",
    "        filenames.......list of review file names\n",
    "        tokenizer_fn....the function used to tokenize each document\n",
    "        min_df..........remove terms from the vocabulary that don't appear\n",
    "                        in at least this many documents\n",
    "        max_df..........remove terms from the vocabulary that appear in more\n",
    "                        than this fraction of documents\n",
    "        binary..........If true, each documents is represented by a binary\n",
    "                        vector, where 1 means a term occurs at least once in \n",
    "                        the document. If false, the term frequency is used instead.\n",
    "        ngram_range.....A tuple (n,m) means to use phrases of length n to m inclusive.\n",
    "                        E.g., (1,2) means consider unigrams and bigrams.\n",
    "    Return:\n",
    "        A tuple (X, vec), where X is the csr_matrix of feature vectors,\n",
    "        and vec is the CountVectorizer object.\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(input = 'filename', tokenizer = tokenizer_fn, min_df = min_df, max_df = max_df, binary = binary, ngram_range  = ngram_range, dtype = int)\n",
    "    X = vec.fit_transform(filenames)\n",
    "    \n",
    "    return (X, vec)\n",
    "    \n",
    "matrix, vec = do_vectorize(all_train_files, binary = False)\n",
    "print ('matrix represents %d documents with %d features' % (matrix.shape[0], matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do not modify. This creates a LogsticRegression object, which\n",
    "# you will use in the do_cross_validation method below.\n",
    "def get_clf():\n",
    "    from sklearn import svm\n",
    "    return svm.SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.array([l.strip() for l in open(\"train/train_y.txt\", 'r').readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def repeatable_random(seed):\n",
    "    hash = str(seed)\n",
    "    while True:\n",
    "        hash = hashlib.md5(hash).digest()\n",
    "        for c in hash:\n",
    "            yield ord(c)\n",
    "\n",
    "def repeatable_shuffle(X, y, filenames):\n",
    "    r = repeatable_random(42) \n",
    "    indices = sorted(range(X.shape[0]), key=lambda x: next(r))\n",
    "    return X[indices], y[indices], np.array(filenames)[indices]\n",
    "\n",
    "X, y, filenames = repeatable_shuffle(matrix, labels, all_train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_cross_validation(X, y, n_folds=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform n-fold cross validation, calling get_clf() to train n\n",
    "    different classifiers. Use sklearn's KFold class: http://goo.gl/wmyFhi\n",
    "    Be sure not to shuffle the data, otherwise your output will differ.\n",
    "    Params:\n",
    "        X.........a csr_matrix of feature vectors\n",
    "        y.........the true labels of each document\n",
    "        n_folds...the number of folds of cross-validation to do\n",
    "        verbose...If true, report the testing accuracy for each fold.\n",
    "    Return:\n",
    "        the average testing accuracy across all folds.\n",
    "    \"\"\"\n",
    "    cv = KFold(len(y), n_folds)\n",
    "    accuracies = []\n",
    "    n = 0\n",
    "    for train_idx, test_idx in cv:\n",
    "        clf = get_clf()\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        predicted = clf.predict(X[test_idx])\n",
    "        acc = accuracy_score(y[test_idx], predicted)\n",
    "        accuracies.append(acc)\n",
    "        if verbose:\n",
    "            print('fold %d accuracy=%.4f' %(n, acc))\n",
    "        n += 1\n",
    "    return np.mean(accuracies)\n",
    "    \n",
    "print('average cross validation accuracy=%.4f' %\n",
    "      do_cross_validation(X, y, n_folds=5, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predictedLabels = []\n",
    "clf = get_clf()\n",
    "clf.fit(X, y)\n",
    "m = clf.predict_proba(X)\n",
    "for pos, neg in m:\n",
    "    if pos > neg:\n",
    "        predictedLabels.append('BEARISH')\n",
    "    else:\n",
    "        predictedLabels.append('BULLISH')\n",
    "        \n",
    "print(metrics.classification_report(y, np.array(predictedLabels)))\n",
    "print(metrics.confusion_matrix(y, np.array(predictedLabels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
